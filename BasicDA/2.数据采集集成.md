# 数据采集集成
`2022年3月15日`

## 数据采集简介


### 大数据的来源
1. 按照来源类型
   - 业务数据： 消费者数据， 客户关系数据等。 
   - 行业数据： 流量数据， 农业， 医疗， 天气环境数据。
   - 内容数据： 应用日志， 电子文档， 机器数据。， 多模态数据（音频， 视频等）。  
   - 线上行为数据： 页面数据， 交互数据， 反馈数据。 
   - 线下行为数据： 物体运动数据， 用户位置和轨迹数据。

2. 按照来源系统
   - 企业系统： ERP, 商户销售系统， 计费账务系统， 财务系统。
   - 机器系统： 智能仪表， 工业设备传感器。 
   - 互联网系统： 电商系统， 政府监管系统。
   - 社交系统： 微信， QQ, 微博等。 

#### 传统数据采集和大数据采集的区别
1. 传统数据是结构化或者半结构化的， 且多采用人为采集。
2. 大数据是非结构化的， 数据产生快， 使用机器采集。 

### 数据采集的三大要点
1. 全面性
   - 数据量足够具有分析价值， 数据面足够支撑分析需求。 
2. 多维性
   - 数据更重要的是能满足分析需求，灵活自定义数据的多种属性和类型， 从而满足不同的分析目标。
3. 高效性
   - 包含技术执行的高效性， 团队协作的高效性， 数据分析需求和目标实现的高效性。

### 大数据的采集方法

1. 数据库采集
2. 系统日志采集
   - 满足高可用性， 高可靠性， 高可扩展性。 
3. 网络数据采集
4. 感知设备数据采集

### 大数据的存储方式
1. 机构化数据
   - 来自业务系统关系数据库， 可使用大数据平台 hive, maxcomputer 来处理， 要对其进行同意仓库建模， 划分层次， 主题。 
2. 半结构化数据
   - 来自日志， 网络， 使用 hive, maxcomputer, nosql 数据库处理， 解析抽取出结构化信息， 进入数据库。
3. 非结构化数据集
   - 来自网络， 检测设备， 使用分布式文件系统 HDFS, OSS, MongoDB 处理， 识别抽取出结构化信息， 进入数据库。 


## 网络数据的采集

### 网络爬虫的概念及应用范围
> 网络爬虫是一种按照一定规则自动爬取网络信息的程序或者脚本。

1. 应用范围
   - 数据采集
   - Web挖掘
   - 舆情分析: 使用的是面向主题的爬虫， 研究方向是`行业领域的搜索策略问题`
   - 离线浏览

### 爬虫分类
1. 通用网络爬虫
   - 对数据不加区分， 无差别的收录， 主要用于搜索引擎， 对性能要求较高。 
2. 聚焦网络爬虫
   - 按照预先定义好的主题有选择的进行网页爬取， 节省带宽和服务器资源。
   -  相关策略
      -  基于内容评价的爬行策略： 引入文本内容相似度的概念， 但无法页面和主题内容相关度的高低。
      -  基于链接结构评价的爬行策略： 如搜索引擎排序。
      -  基于增强学习的爬行策略： 将增强学习引入聚焦爬虫， 根据整个网页文本和链接文本对链接进行分类， 以此决定链接爬取的顺序。 
      -  基于语境图的爬行策略： 建立语境图， 学习网页之间的相关度

3. 增量式网络爬虫
   - 对已经下载的内容进行增量式爬取， 并非周期爬取， 仅在数据更新时才进行抓取， 抓取尽可能新的页面
   - 实现起来比较困难， 对算法要求较高。

4. 深层网络爬虫
   - 对网络深层页面进行爬取。 表层页面指的是不需要表单的静态页面， 深层页面则是动态的， 藏在表单之后的。 
   - 最重要的部分是`表单处理`。 

### 常见网络搜索策略
1. 深度优先： 处理完该URL中的全部页面， 再处理下一条URL。 
2. 广度优先： 完成当前页面的搜索后， 才进行下层页面的搜索， 主要目标是收集尽可能多的页面。 
3. 最佳优先： 在众多的URL中选择评价最好的进行抓取。因为采用的是局部最优策略，因此可能漏掉一部分内容。

### 基于机器学习的网页抽取技术
1. 基于启发式规则的无监督学习的网页抽取算法。
   - 将网页解析为 tocken 序列， 从tocken序列中找到一个子序列， 使得子序列中tocken的得分最大, 则子序列就是正文。 即：从源码字符串中， 找到一个同时含有最多文本和最少标签的字符串。
   - 使用无监督学习 如聚类算法将网页分为正文和非正文两部分。 
2. 基于分类器的网页抽取算法
  1. 找到几千个经过标注正文和非正文的网页作为训练集
  2. 设计特征， 如不同的标签类型。 
  3. 选择合适的分类器， 利用特征进行训练
3. 基于网页模板自动生成的网页抽取算法。
   - 找到多个相同结构的页面（通过URL判断）， 对比其中的异同， 页面的共性部分是非正文，而差别较大的则是正文。 

### Robots协议
> Robots Exclusion Protocol

## 网络爬虫实现

### 爬取流程

1. 寻找数据源
2. 确定许可
3. 却定工具
4. 访问地址
5. 解析文档
6. 采集数据


#  系统日志采集

### Flume 系统日志采集
Event是Flume数据流的基本单位。 Agent 是Flume中最小的独立运行单位，

Agent 包含： Source， Channel, Sink


## 数据同步

### 什么是数据同步
- 数据同步是为了保持源与目的数据一致性而进行的数据传输， 处理的过程。
  
### 数据同步方式
- 直连同步
  - 通过预定义的规范接口API和基于动态链接库的方式直接连接业务库。
  - 优点： 配置简单， 实现容易， 比较适合操作型业务系统的数据同步。
  - 缺点： 对源数据的性能影响较大， 执行大批量数据同步时会降低业务系统的性能。 
- 数据文件同步
  - 从源系统生成数据的文本文件加载到目标数据库系统中。
  - 优点： 当数据源包含多个异构的数据库时， 该方法比较适用。
- 数据库日志解析同步
  - 通过解析日志文件获取发生变更的数据， 从而满足增量数据同步的需求。
  - 优点： 性能好， 效率高， 实现了实时与准实时同步的能力。
  - 缺点： 投入较大， 数据漂移和遗漏。 

> 数据漂移：接入层表同一个业务日期数据中包含前一天或者后一天凌晨附近的数据或者丢失当天的变更数据。

### 常用数据同步工具
1. 离线数据同步工具
   - Sqoop: 主要用于在Hadoop与关系型数据库之间进行数据同步。
   - Kettle: 开源ETL工具， 数据抽取高效稳定。
   - DataX: 阿里数据同步产品， 异构数据源同步工具。
2. 实时数据同步工具
   - OGG: 跨平台， 异构数据库之间的数据同步工具。
   - Debezum: 监控数据库的日志变化
   - DTS: 阿里云数据同步工具。 


[返回](../index.md)

