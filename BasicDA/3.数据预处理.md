
# 数据预处理

## 概论

### 必要性

现实世界中数据的特点
- 数据不完整
- 数据有错误
- 数据有噪声
- 数据不一致
- 数据有重复

数据的结构
- 前台： 指具体的数据应用， 重点关注客户应用场景，解决企业消费者的实际问题。
- 中台： 指通过数据技术， 对海量数据进行采集，计算，存储，加工， 同时统一标准和口径。
- 后台： 指存储和计算，重点关注数据的处理速度和成本。


### 主要任务

> 数据预处理： 对数据进行挖掘之前， 先对原始数据进行必要的清洗，集成，转换，离散和规约等一些列的处理工作，以期达到挖掘算法进行知识获取研究要求的最低规范和标准。

#### 数据清洗

- 缺失值处理
- 噪声处理
- 离群点处理


#### 数据集成

- 集成数据源
- 解决歧义问题
- 解决数据不一致性
- 解决数据冗余



#### 数据变换

主要任务是找到数据的特征表示包含

- 数据平滑
- 数据规范化
- 数据离散化
- 属性构造
- 概念分层


#### 数据规约

> 将数据按照语义层结构合并，语义层此结构定义了元组属性值之间的语义信息

- 维归约
- 数量规约
- 数据压缩

## 数据清洗

### 数据清洗的主要对象

#### 缺失数据

数据缺失带来的问题
- 系统可能丢失了大量的有用信息
- 系统中所表现出的不确定性更加显著
- 使挖掘过程陷入混乱， 导致不可靠的输出

缺失方式的划分
- 完全随机缺失： 缺失与数据集中其他数据的取值或缺失没有关系，从统计意义上来说，这种缺失是独立的。 e.g. 采集设备故障
- 随机缺失： 缺失与缺失的数据值无关，但与数据在某些属性上的取值有关。 e.g. 人们是否透露收入的可能性与性别，教育程度，职业等因素有关。
- 非随机缺失： 缺失值与自身属性取值有关 e.g. 某特定人群补不填写收入值，不可忽略， 如 儿童的固定收入

处理方式

删除法
- 适用于缺失值较多且该变量对研究的问题不太重要，一般可采用元组删除，变量删除，成对删除

估计法
- 人工填充
- 特殊值填充： 使用一个全局常量来填充空缺值
- 使用属性的中心度量
  - 对称的数据采用均值填充
  - 倾斜分布的数据采用中位数、条件填充
- 使用最可能的值填充
  - 回归
  - 贝叶斯
  - 决策树
  - K近邻
  - EM

不处理
- 防止主观估计值引起新噪声的产生。


根据缺失值的缺失率和重要性来确定处理策略

||重要性低|重要性高|
|:----:|:----:|:----:|
|缺失率低|不做处理或进行简单填充|通过计算进行填充<br>通过经验或业务知识估计|
|缺失率高|去除变量|尝试从其他渠道取数补全<br>使用其他变量通过计算获取<br>去除变量并在结果中标明|


#### 噪声数据

指在测量时测量值可能出现的相对于真实值的偏差或错误， 会影响后续分析操作的结果。

错误数据：指数据源环境中经常出现的一类问题， 主要包括
- 数据值错误
- 数据类型错误： e.g. 日期存储为数值， 时间戳存储为字符串
- 数据编码错误
- 数据格式错误
- 数据异常错误：e.g. 全半角错误，日期越界， 不可见字符等
- 依赖冲突：e.g. 省和市不匹配
- 多值错误

#### 异常值处理

指数据集中存在不合理的个别值，其数据明显偏离所属样本的其余观测值

识别方式： $3 \sigma$ 原则。

异常值的处理方式：
- 删除
- 视为缺失值
- 平滑处理： 分箱，聚类，回归等
- 不处理

#### 重复处理

数据集中可能存在重复字段或记录， 基本处理思想是 排序和合并

排序算法

相似度计算方法
- 基本的字段匹配算法
- 标准欧式距离
- 汉明距离
- 夹角余弦
- 杰卡德距离
- 马氏距离
- 曼哈顿距离
- 闵可夫斯基距离
- 欧式距离
- 切比雪夫距离
- 相关系数
- 信息熵

## 数据清洗的示例

```python
import numpy as np
import pandas as pd
import seaborn as sns # for visualization


Data_Path = ''
ColumnsName = []

data = pd.read_csv(Data_Path, names=ColumnsName)

data.head() # check the data head
data.info() # check the columns, None-Null count, dtype, shape e.t.c.


# deal with miss value
data = data.replace(to_replace="?", value=np.nan) # replace the none value

data = data.dropna(how='any') # drop the data row with NaN.
data.to_csv("new_data_name.csv", index=0)


# find the unusual data via box chart, if data with Normal distribution then use Normal distribution chart with 3 sigma.
sns.boxplot(x=data["Class"], y = data["Clump Thickness"])

Q1 = data["Nland Chromatin"].quantile(q=.25)
Q3 = data["Nland Chromatin"].quantile(q=.75)

# base the 1.5 times diff of quantile to calculate the boundary
low_boundry = Q1 - 1.5*(Q3-Q1)
up_boundry = Q3 + 1.5*(Q3-Q1)

data["Bland Chormatin"].query("`Bland Chormatin` > @low_boundry and `Bland Chormatin` < @up_boundry")

```

## 数据集成

> 数据集成是技术流程和业务流程的组合， 用于将不同来源的数据组合成有意义有价值的信息。

### 传统数据集成

用于解决语义歧义，实例歧义和数据不一致性带来的问题

流程：
1. 模式对齐： 解决语义歧义带来的问题即 某个属性在多表中的冗余问题。
2. 记录链接： 解决哪些记录表示的是相同的实体而哪些不是
3. 数据融合： 针对数据质量带来的问题， 即不同的数据源相互冲突时， 采用哪个值。


### 异构数据集成

主要分为以下几种
- 结构异构： 不同数据源数据结构的不同
  - 可采用 模式集成或数据复制（ETL）的方法
- 语义性异构： 不同数据源的数据项在含义上有差别
  - 可采用基于本体的数据集成
- 数据源的异地分布性
- 数据源的自治性 ？？？？

## 数据规约

> 在大数据上进行复杂的数据分析和挖掘需要很长时间， 数据规约产生更小但是保持原始数据完整性的新数据集。 在规约后的数据集上进行分析和挖掘将更有效率。

数据规约的意义在于：
- 降低无效，错误的数据对建模的影响，提高建模的准确性。
- 少量且具代表性的数据将大幅缩减数据挖掘所需要的时间。
- 降低存储数据的成本。


### 维度规约

> 从原有的数据中， 通过减少数据集中的属性或者使用组合属性代替原属性来精简数据集， 目的是减少系统资源的消耗并降低无效/错误数据的影响

数据降维
- 特征选择： 从原始特征集中选出一组具有统计意义的特征，以降低数据集中的属性数，不会改变原始特征空间
  - 常见的方法： 决策树，分支定界法，序列前向选择，序列后向选择，模拟退火，竞技搜索，遗传算法。
- 特征提取：将原始特征转换为一组具有明显物理意义或者统计意义的特征，以降低数据集中的属性数。方式会通过转换原有特征得到新的特征，因此会改变原特征空间。
  - 常见的方式有： 主成分分析，线性判别分析，奇异值分解，独立成分分析。


### 数量规约

> 通过减少数据集中的数据量减小数据集

数量规约的方法
- 参数化数据归约
  - 线性回归， 多元回归
  - 对数线性模型：用于近似离散属性集中的多维概率分布。
- 非参数化数据规约
  - 直方图
  - 聚类
  - 抽样

### 数据压缩

无损压缩： 压缩后的数据可以重构原始数据

有损压缩： 压缩后的数据不足以重构原始的数据。


## 数据规约示例

```python
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt


daily = pd.read_csv("", header="infer")# ("", pase_date="DATE")
daily.head()

daily.info()

daily.index = pd.to_datetime(daily.DATE) # convert the str date to datetime type and set it to index. 


monthly = daily.groupby(pd.Grouper(freq='M')).sum()
ax = monthly.plot(kind='line', figsize=(15, 3))
ax.set_title('Monthly Precipitation')

```

## 数据变化

### 属性构造

> 有给定的属性构造先的属性， 并加入到现有的属性集合中， 以增加对数据高维信息的理解和精确度。


### 规范化

> 将有关属性数据按比例投射到特定的小范围之中。 目的是为了消除属性之间的量纲和取值范围差异的影响。

常用方式
- 最小最大规范化 
  - $$v'=\frac{v-min_{A}}{max_A-min_A}\left(newmax_A-newmin_A\right)+newmin_A$$
- z-score 规范化
  - $$v'=\frac{v-EA}{\sigma_A}$$
- 小数定标
  - $$v'=\frac{v}{10^j}$$


### 离散化

> 将数值型变量变成离散型变量的过程

数据离散化的方法
- 无监督离散化方法
  - 等宽分箱
  - 等深分箱
  - 聚类方法
- 有监督离散化方法
  - 1R方法
  - 基于熵的离散化方法
  - 卡方分裂算法


### 数据聚集

### 概念分层

> 用来将数据变换到多个抽象层， 来取代低层次数据层的数据对象。

如 国家-省-市-区-街道 可以针对不同的抽象层次采用不同的算法


## 数据变化示例

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder # num encoding
from sklearn.preprocessing import OneHotEncoder # one-hot encoding
from sklearn.feature_extraction import DictVectorizer 



data = pd.read_csv("", encoding='gbk') # simplify chinese

df_data_oe = data.copy() # copy is important


oe = OrdinalEncoder()
oe.fit(df_data.iloc[:,:7].values.reshape(-1, 7)).categories_
oe.fit_transform(df_data.iloc[:,:7].values.reshape(-1, 7))
df_data_oe.iloc[:,:7] = oe.fit_transform(df_data.iloc[:,:7].values.reshape(-1, 7))

```
[返回](../index.md)

